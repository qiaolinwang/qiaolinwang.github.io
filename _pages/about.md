---
layout: about
title: about
permalink: /
subtitle: qw2443@columbia.edu | <a href="https://www.ee.columbia.edu/" target="_blank">Columbia EE</a>

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular

selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: true # adds a vertical scroll bar if there are more than 3 news items
  limit: 5 # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am Qiaolin Wang, a second-year Master's student in Electrical Engineering at Columbia University, advised by <a href="https://nima.ee.columbia.edu/" target="_blank">Professor Nima Mesgarani</a>.

I am passionate about building models that can <strong>listen, understand, and speak</strong> as naturally as humans do. My current research investigates the fundamental capabilities of <strong>Large Audio Language Models (LALMs)</strong>, from their <a href="https://arxiv.org/pdf/2509.15655" target="_blank">internal representations about syntax and context</a>, to their capacity for <a href="https://arxiv.org/pdf/2509.15661" target="_blank">complex reasoning across modalities</a>. My future work aims to advance this by pioneering deeper <strong>audio-visual perception alignment</strong> and unified models for <strong>multimodal reasoning and generation</strong>.

Before joining Columbia, I earned my B.Eng. in Computer Science from Wuhan University. I also had an enriching experience as a Research Intern at <a href="https://www.wiz.ai/" target="_blank">Wiz.AI</a>, where I developed a SOTA Speech Emotion Recognition LLM and deployed production-line text-to-speech systems.
